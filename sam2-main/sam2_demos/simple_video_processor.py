#!/usr/bin/env python3
"""
SAM2 çœŸå®è§†é¢‘å¤„ç†åº”ç”¨ - ç®€åŒ–ç‰ˆ
ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…å¤æ‚ä¾èµ–
"""

import os
import numpy as np
from PIL import Image, ImageDraw
import cv2
import json
from datetime import datetime

class SimpleVideoProcessor:
    def __init__(self):
        """åˆå§‹åŒ–ç®€å•è§†é¢‘å¤„ç†å™¨"""
        self.supported_formats = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']
        print("ğŸ¥ SAM2 ç®€åŒ–è§†é¢‘å¤„ç†å™¨å·²åˆå§‹åŒ–")
    
    def create_demo_video(self, output_path="sam2_demos/demo_video.mp4", duration=3, fps=5):
        """åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¼”ç¤ºè§†é¢‘"""
        print(f"ğŸ¬ åˆ›å»ºæ¼”ç¤ºè§†é¢‘: {output_path}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # è§†é¢‘å‚æ•°
        width, height = 320, 240
        total_frames = duration * fps
        
        try:
            # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            # ç”Ÿæˆè§†é¢‘å¸§
            for frame_idx in range(total_frames):
                # åˆ›å»ºèƒŒæ™¯
                frame = np.ones((height, width, 3), dtype=np.uint8) * 64
                
                # æ·»åŠ ç§»åŠ¨çš„åœ†å½¢ç›®æ ‡
                center_x = int(50 + (frame_idx / total_frames) * (width - 100))
                center_y = int(height // 2 + 30 * np.sin(frame_idx * 0.5))
                cv2.circle(frame, (center_x, center_y), 20, (0, 255, 0), -1)
                
                # æ·»åŠ é™æ€çš„æ–¹å½¢ç›®æ ‡
                rect_x, rect_y = width - 60, 60
                cv2.rectangle(frame, (rect_x - 15, rect_y - 15), 
                             (rect_x + 15, rect_y + 15), (255, 0, 0), -1)
                
                # æ·»åŠ å¸§å·æ–‡æœ¬
                cv2.putText(frame, f"F{frame_idx+1:02d}", (10, 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)
                
                out.write(frame)
            
            out.release()
            print(f"âœ… æ¼”ç¤ºè§†é¢‘å·²åˆ›å»º: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºè§†é¢‘å¤±è´¥: {e}")
            return None
    
    def extract_video_frames(self, video_path, output_dir, max_frames=10):
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        print(f"ğŸ“¹ ä»è§†é¢‘æå–å¸§: {video_path}")
        
        if not os.path.exists(video_path):
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return []
        
        os.makedirs(output_dir, exist_ok=True)
        
        try:
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.1f} FPS")
            
            frame_paths = []
            frame_interval = max(1, total_frames // max_frames)
            
            frame_idx = 0
            extracted_count = 0
            
            while cap.isOpened() and extracted_count < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_idx % frame_interval == 0:
                    frame_path = os.path.join(output_dir, f"frame_{extracted_count:03d}.jpg")
                    cv2.imwrite(frame_path, frame)
                    frame_paths.append(frame_path)
                    extracted_count += 1
                
                frame_idx += 1
            
            cap.release()
            print(f"âœ… æˆåŠŸæå– {len(frame_paths)} å¸§")
            return frame_paths
            
        except Exception as e:
            print(f"âŒ æå–å¸§å¤±è´¥: {e}")
            return []
    
    def analyze_video_motion(self, frame_paths):
        """åˆ†æè§†é¢‘ä¸­çš„è¿åŠ¨"""
        print("ğŸ” åˆ†æè§†é¢‘è¿åŠ¨...")
        
        if len(frame_paths) < 2:
            print("âŒ éœ€è¦è‡³å°‘2å¸§è¿›è¡Œè¿åŠ¨åˆ†æ")
            return {}
        
        try:
            motion_data = {
                "total_frames": len(frame_paths),
                "motion_detected": [],
                "object_positions": []
            }
            
            prev_frame = None
            
            for i, frame_path in enumerate(frame_paths):
                frame = cv2.imread(frame_path)
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                if prev_frame is not None:
                    # è®¡ç®—å¸§å·®
                    diff = cv2.absdiff(prev_frame, gray)
                    _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)
                    
                    # æ£€æµ‹è¿åŠ¨
                    motion_pixels = np.sum(thresh > 0)
                    motion_ratio = motion_pixels / (thresh.shape[0] * thresh.shape[1])
                    
                    motion_data["motion_detected"].append({
                        "frame": i,
                        "motion_ratio": float(motion_ratio),
                        "motion_pixels": int(motion_pixels)
                    })
                
                # ç®€å•çš„ç»¿è‰²å¯¹è±¡æ£€æµ‹ (æ¼”ç¤ºç”¨)
                hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
                green_mask = cv2.inRange(hsv, (40, 100, 100), (80, 255, 255))
                
                # æ‰¾åˆ°ç»¿è‰²å¯¹è±¡çš„è´¨å¿ƒ
                moments = cv2.moments(green_mask)
                if moments['m00'] > 0:
                    cx = int(moments['m10'] / moments['m00'])
                    cy = int(moments['m01'] / moments['m00'])
                    motion_data["object_positions"].append({
                        "frame": i,
                        "x": cx,
                        "y": cy
                    })
                
                prev_frame = gray
            
            print(f"âœ… è¿åŠ¨åˆ†æå®Œæˆ: {len(motion_data['motion_detected'])} å¸§è¿åŠ¨æ•°æ®")
            return motion_data
            
        except Exception as e:
            print(f"âŒ è¿åŠ¨åˆ†æå¤±è´¥: {e}")
            return {}
    
    def create_motion_visualization(self, motion_data, output_path):
        """åˆ›å»ºè¿åŠ¨åˆ†æå¯è§†åŒ–"""
        print(f"ğŸ“Š åˆ›å»ºè¿åŠ¨å¯è§†åŒ–: {output_path}")
        
        try:
            import matplotlib.pyplot as plt
            
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
            
            # è¿åŠ¨å¼ºåº¦å›¾
            if motion_data.get("motion_detected"):
                frames = [d["frame"] for d in motion_data["motion_detected"]]
                ratios = [d["motion_ratio"] for d in motion_data["motion_detected"]]
                
                ax1.plot(frames, ratios, 'b-o', markersize=4)
                ax1.set_title("è§†é¢‘è¿åŠ¨å¼ºåº¦åˆ†æ", fontsize=14)
                ax1.set_xlabel("å¸§å·")
                ax1.set_ylabel("è¿åŠ¨æ¯”ä¾‹")
                ax1.grid(True, alpha=0.3)
            
            # å¯¹è±¡è½¨è¿¹å›¾
            if motion_data.get("object_positions"):
                x_pos = [p["x"] for p in motion_data["object_positions"]]
                y_pos = [p["y"] for p in motion_data["object_positions"]]
                frames = [p["frame"] for p in motion_data["object_positions"]]
                
                scatter = ax2.scatter(x_pos, y_pos, c=frames, cmap='viridis', s=50)
                ax2.plot(x_pos, y_pos, 'r-', alpha=0.5, linewidth=2)
                ax2.set_title("ç»¿è‰²ç›®æ ‡è¿åŠ¨è½¨è¿¹", fontsize=14)
                ax2.set_xlabel("X åæ ‡")
                ax2.set_ylabel("Y åæ ‡")
                plt.colorbar(scatter, ax=ax2, label="å¸§å·")
                ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… è¿åŠ¨å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå¯è§†åŒ–å¤±è´¥: {e}")
            return False
    
    def demo_simple_video_processing(self):
        """æ¼”ç¤ºç®€å•è§†é¢‘å¤„ç†åŠŸèƒ½"""
        print("ğŸš€ ç®€å•è§†é¢‘å¤„ç†æ¼”ç¤º")
        print("=" * 50)
        
        try:
            # 1. åˆ›å»ºæ¼”ç¤ºè§†é¢‘
            video_path = self.create_demo_video()
            if not video_path:
                print("âŒ æ— æ³•åˆ›å»ºæ¼”ç¤ºè§†é¢‘")
                return
            
            # 2. æå–è§†é¢‘å¸§
            frames_dir = "sam2_demos/simple_video_frames"
            frame_paths = self.extract_video_frames(video_path, frames_dir, max_frames=8)
            
            if not frame_paths:
                print("âŒ æ— æ³•æå–è§†é¢‘å¸§")
                return
            
            # 3. åˆ†æè¿åŠ¨
            motion_data = self.analyze_video_motion(frame_paths)
            
            # 4. åˆ›å»ºå¯è§†åŒ–
            viz_path = "sam2_demos/simple_video_motion_analysis.png"
            self.create_motion_visualization(motion_data, viz_path)
            
            # 5. ä¿å­˜åˆ†ææŠ¥å‘Š
            self.create_analysis_report(video_path, frame_paths, motion_data)
            
            print("\nâœ… ç®€å•è§†é¢‘å¤„ç†æ¼”ç¤ºå®Œæˆ!")
            print("ğŸ“ æŸ¥çœ‹ç»“æœ:")
            print(f"  - æ¼”ç¤ºè§†é¢‘: {video_path}")
            print(f"  - æå–å¸§: {frames_dir}")
            print(f"  - è¿åŠ¨åˆ†æ: {viz_path}")
            print(f"  - åˆ†ææŠ¥å‘Š: sam2_demos/simple_video_analysis_report.md")
            
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def create_analysis_report(self, video_path, frame_paths, motion_data):
        """åˆ›å»ºåˆ†ææŠ¥å‘Š"""
        report_content = f"""# SAM2 ç®€å•è§†é¢‘å¤„ç†åˆ†ææŠ¥å‘Š

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æºè§†é¢‘**: {os.path.basename(video_path)}
- **æå–å¸§æ•°**: {len(frame_paths)}

## ğŸ¥ è§†é¢‘åˆ†æç»“æœ

### è¿åŠ¨æ£€æµ‹
- **æ£€æµ‹å¸§æ•°**: {len(motion_data.get('motion_detected', []))}
- **å¹³å‡è¿åŠ¨å¼ºåº¦**: {np.mean([d['motion_ratio'] for d in motion_data.get('motion_detected', [])]) if motion_data.get('motion_detected') else 0:.3f}

### ç›®æ ‡è·Ÿè¸ª
- **è·Ÿè¸ªå¸§æ•°**: {len(motion_data.get('object_positions', []))}
- **è½¨è¿¹å®Œæ•´æ€§**: {len(motion_data.get('object_positions', []))/len(frame_paths)*100:.1f}%

## ğŸ”§ æŠ€æœ¯ç‰¹ç‚¹
- **è½»é‡çº§å¤„ç†**: ä½¿ç”¨OpenCVè¿›è¡ŒåŸºç¡€è§†é¢‘åˆ†æ
- **è¿åŠ¨æ£€æµ‹**: åŸºäºå¸§å·®çš„è¿åŠ¨æ£€æµ‹ç®—æ³•
- **ç›®æ ‡è·Ÿè¸ª**: åŸºäºé¢œè‰²çš„ç®€å•ç›®æ ‡æ£€æµ‹
- **å¯è§†åŒ–åˆ†æ**: matplotlibç”Ÿæˆåˆ†æå›¾è¡¨

## ğŸ’¡ åº”ç”¨åœºæ™¯
- **ç›‘æ§åˆ†æ**: åŸºç¡€è¿åŠ¨æ£€æµ‹å’Œç›®æ ‡è·Ÿè¸ª
- **è§†é¢‘é¢„å¤„ç†**: ä¸ºSAM2åˆ†å‰²åšå‡†å¤‡
- **æ•™å­¦æ¼”ç¤º**: è®¡ç®—æœºè§†è§‰åŸºç¡€æ¦‚å¿µå±•ç¤º
- **å¿«é€ŸåŸå‹**: è§†é¢‘åˆ†æåº”ç”¨åŸå‹å¼€å‘

## ğŸš€ æ‰©å±•æ–¹å‘
- **é›†æˆSAM2**: æ·»åŠ ç²¾ç¡®çš„å®ä¾‹åˆ†å‰²
- **å¤šç›®æ ‡è·Ÿè¸ª**: æ”¯æŒå¤šä¸ªç›®æ ‡åŒæ—¶è·Ÿè¸ª
- **èƒŒæ™¯å»ºæ¨¡**: æ›´å¤æ‚çš„èƒŒæ™¯åˆ†ç¦»æŠ€æœ¯
- **ç‰¹å¾è·Ÿè¸ª**: åŸºäºç‰¹å¾ç‚¹çš„ç›®æ ‡è·Ÿè¸ª

è¿™ä¸ªç®€åŒ–ç‰ˆæœ¬ä¸ºåç»­é›†æˆSAM2é«˜çº§åŠŸèƒ½å¥ å®šäº†åŸºç¡€ï¼
"""
        
        report_path = "sam2_demos/simple_video_analysis_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def demo_simple_video_applications():
    """è¿è¡Œç®€å•è§†é¢‘åº”ç”¨æ¼”ç¤º"""
    try:
        processor = SimpleVideoProcessor()
        processor.demo_simple_video_processing()
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")


if __name__ == "__main__":
    demo_simple_video_applications()
