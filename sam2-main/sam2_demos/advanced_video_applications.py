#!/usr/bin/env python3
"""
SAM2 é«˜çº§è§†é¢‘åˆ†å‰²åº”ç”¨
åŒ…å«ç›®æ ‡æå–ã€èƒŒæ™¯æ›¿æ¢ã€ç›®æ ‡è·Ÿè¸ªç­‰é«˜çº§åŠŸèƒ½
"""

import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFilter
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import os
import cv2
import json
from datetime import datetime
from sam2.build_sam import build_sam2_video_predictor
from sam2.sam2_video_predictor import SAM2VideoPredictor

class AdvancedVideoSegmenter:
    def __init__(self, model_path="checkpoints/sam2.1_hiera_tiny.pt"):
        """åˆå§‹åŒ–é«˜çº§è§†é¢‘åˆ†å‰²å™¨"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ åˆå§‹åŒ–SAM2é«˜çº§è§†é¢‘åˆ†å‰²å™¨ (è®¾å¤‡: {self.device})")
        
        # åŠ è½½è§†é¢‘é¢„æµ‹å™¨
        self.predictor = SAM2VideoPredictor.from_pretrained(
            model_id="facebook/sam2-hiera-tiny",
            device=self.device
        )
        print("âœ… è§†é¢‘æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # è·Ÿè¸ªçŠ¶æ€
        self.tracking_objects = {}
        self.tracking_history = {}
    
    def extract_frames(self, video_path, output_dir, max_frames=30):
        """ä»è§†é¢‘æå–å¸§"""
        os.makedirs(output_dir, exist_ok=True)
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.1f} FPS")
        
        # è®¡ç®—é‡‡æ ·é—´éš”
        frame_interval = max(1, total_frames // max_frames)
        extracted_frames = []
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                # è½¬æ¢ä¸ºRGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_path = os.path.join(output_dir, f"frame_{len(extracted_frames):04d}.jpg")
                Image.fromarray(frame_rgb).save(frame_path)
                extracted_frames.append(frame_path)
                
                if len(extracted_frames) >= max_frames:
                    break
            
            frame_count += 1
        
        cap.release()
        print(f"âœ… æå–äº† {len(extracted_frames)} å¸§")
        return extracted_frames, fps
    
    def target_extraction(self, frame_paths, click_point, output_dir, obj_name="target"):
        """
        ç›®æ ‡æå–åŠŸèƒ½ï¼šä»è§†é¢‘ä¸­æå–ç‰¹å®šç›®æ ‡
        """
        print(f"ğŸ¯ å¼€å§‹ç›®æ ‡æå–: {obj_name}")
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
        inference_state = self.predictor.init_state(video_path=os.path.dirname(frame_paths[0]))
        
        # åœ¨ç¬¬ä¸€å¸§æ·»åŠ ç‚¹å‡»ç‚¹
        points = np.array([click_point], dtype=np.float32)
        labels = np.array([1], np.int32)
        
        _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=0,
            obj_id=1,
            points=points,
            labels=labels,
        )
        
        # ä¼ æ’­åˆ°æ‰€æœ‰å¸§å¹¶æå–ç›®æ ‡
        extracted_targets = []
        target_masks = {}
        
        for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(inference_state):
            frame = Image.open(frame_paths[out_frame_idx])
            frame_array = np.array(frame)
            
            # è·å–æ©ç 
            mask = (out_mask_logits[0] > 0.0).cpu().numpy()
            target_masks[out_frame_idx] = mask
            
            # åˆ›å»ºé€æ˜èƒŒæ™¯çš„ç›®æ ‡å›¾åƒ
            target_image = self._extract_target_with_transparency(frame_array, mask)
            
            # ä¿å­˜æå–çš„ç›®æ ‡
            target_path = os.path.join(output_dir, f"{obj_name}_frame_{out_frame_idx:04d}.png")
            target_image.save(target_path)
            extracted_targets.append(target_path)
        
        # åˆ›å»ºæå–ç»“æœæ±‡æ€»
        self._create_extraction_summary(frame_paths, extracted_targets, target_masks, output_dir, obj_name)
        
        # ç”Ÿæˆæå–æŠ¥å‘Š
        self._generate_extraction_report(frame_paths, target_masks, output_dir, obj_name, click_point)
        
        print(f"âœ… ç›®æ ‡æå–å®Œæˆ: {len(extracted_targets)} å¸§")
        return extracted_targets, target_masks
    
    def background_replacement(self, frame_paths, click_point, new_background_path, output_dir):
        """
        èƒŒæ™¯æ›¿æ¢åŠŸèƒ½ï¼šå°†è§†é¢‘èƒŒæ™¯æ›¿æ¢ä¸ºæ–°èƒŒæ™¯
        """
        print(f"ğŸ¨ å¼€å§‹èƒŒæ™¯æ›¿æ¢")
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½æ–°èƒŒæ™¯
        new_bg = Image.open(new_background_path).convert("RGB")
        
        # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
        inference_state = self.predictor.init_state(video_path=os.path.dirname(frame_paths[0]))
        
        # åœ¨ç¬¬ä¸€å¸§æ·»åŠ ç‚¹å‡»ç‚¹ï¼ˆé€‰æ‹©å‰æ™¯å¯¹è±¡ï¼‰
        points = np.array([click_point], dtype=np.float32)
        labels = np.array([1], np.int32)
        
        _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=0,
            obj_id=1,
            points=points,
            labels=labels,
        )
        
        # ä¼ æ’­å¹¶æ›¿æ¢èƒŒæ™¯
        replaced_frames = []
        replacement_masks = {}
        
        for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(inference_state):
            frame = Image.open(frame_paths[out_frame_idx])
            
            # è·å–å‰æ™¯æ©ç 
            mask = (out_mask_logits[0] > 0.0).cpu().numpy()
            replacement_masks[out_frame_idx] = mask
            
            # æ‰§è¡ŒèƒŒæ™¯æ›¿æ¢
            replaced_frame = self._replace_background(frame, mask, new_bg)
            
            # ä¿å­˜æ›¿æ¢ç»“æœ
            output_path = os.path.join(output_dir, f"replaced_frame_{out_frame_idx:04d}.png")
            replaced_frame.save(output_path)
            replaced_frames.append(output_path)
        
        # åˆ›å»ºæ›¿æ¢æ•ˆæœå¯¹æ¯”
        self._create_replacement_comparison(frame_paths, replaced_frames, replacement_masks, output_dir)
        
        # ç”Ÿæˆæ›¿æ¢æŠ¥å‘Š
        self._generate_replacement_report(frame_paths, replacement_masks, new_background_path, output_dir, click_point)
        
        print(f"âœ… èƒŒæ™¯æ›¿æ¢å®Œæˆ: {len(replaced_frames)} å¸§")
        return replaced_frames, replacement_masks
    
    def object_tracking(self, frame_paths, click_points, output_dir, track_names=None):
        """
        ç›®æ ‡è·Ÿè¸ªåŠŸèƒ½ï¼šå¤šç›®æ ‡è·Ÿè¸ªå’Œè¿åŠ¨åˆ†æ
        """
        print(f"ğŸ” å¼€å§‹å¤šç›®æ ‡è·Ÿè¸ª")
        os.makedirs(output_dir, exist_ok=True)
        
        if track_names is None:
            track_names = [f"object_{i+1}" for i in range(len(click_points))]
        
        # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
        inference_state = self.predictor.init_state(video_path=os.path.dirname(frame_paths[0]))
        
        # ä¸ºæ¯ä¸ªç›®æ ‡æ·»åŠ ç‚¹å‡»ç‚¹
        for i, (click_point, obj_name) in enumerate(zip(click_points, track_names)):
            points = np.array([click_point], dtype=np.float32)
            labels = np.array([1], np.int32)
            
            _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(
                inference_state=inference_state,
                frame_idx=0,
                obj_id=i+1,
                points=points,
                labels=labels,
            )
            
            # åˆå§‹åŒ–è·Ÿè¸ªå†å²
            self.tracking_history[i+1] = {
                'name': obj_name,
                'positions': [],
                'areas': [],
                'confidence_scores': []
            }
        
        # æ‰§è¡Œè·Ÿè¸ª
        tracking_results = {}
        tracked_frames = []
        
        for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(inference_state):
            frame = Image.open(frame_paths[out_frame_idx])
            frame_array = np.array(frame)
            
            frame_results = {}
            
            # å¤„ç†æ¯ä¸ªè·Ÿè¸ªå¯¹è±¡
            for i, (obj_id, mask_logit) in enumerate(zip(out_obj_ids, out_mask_logits)):
                mask = (mask_logit > 0.0).cpu().numpy()
                
                # è®¡ç®—ç›®æ ‡ä½ç½®å’Œå±æ€§
                centroid, area, bbox = self._analyze_object_properties(mask)
                confidence = float(torch.max(mask_logit).cpu())
                
                # æ›´æ–°è·Ÿè¸ªå†å²
                self.tracking_history[obj_id]['positions'].append(centroid)
                self.tracking_history[obj_id]['areas'].append(area)
                self.tracking_history[obj_id]['confidence_scores'].append(confidence)
                
                frame_results[obj_id] = {
                    'mask': mask,
                    'centroid': centroid,
                    'area': area,
                    'bbox': bbox,
                    'confidence': confidence
                }
            
            tracking_results[out_frame_idx] = frame_results
            
            # åˆ›å»ºå¸¦è·Ÿè¸ªä¿¡æ¯çš„å¯è§†åŒ–å¸§
            tracked_frame = self._create_tracking_visualization(frame_array, frame_results, out_frame_idx)
            
            # ä¿å­˜è·Ÿè¸ªç»“æœå¸§
            tracked_path = os.path.join(output_dir, f"tracked_frame_{out_frame_idx:04d}.png")
            Image.fromarray(tracked_frame).save(tracked_path)
            tracked_frames.append(tracked_path)
        
        # åˆ†æè¿åŠ¨è½¨è¿¹
        motion_analysis = self._analyze_motion_trajectories()
        
        # åˆ›å»ºè·Ÿè¸ªåˆ†ææŠ¥å‘Š
        self._create_tracking_analysis(tracking_results, motion_analysis, output_dir)
        
        # ç”Ÿæˆè·Ÿè¸ªæŠ¥å‘Š
        self._generate_tracking_report(tracking_results, motion_analysis, click_points, track_names, output_dir)
        
        print(f"âœ… ç›®æ ‡è·Ÿè¸ªå®Œæˆ: {len(tracked_frames)} å¸§, {len(click_points)} ä¸ªç›®æ ‡")
        return tracked_frames, tracking_results, motion_analysis
    
    def _extract_target_with_transparency(self, frame_array, mask):
        """åˆ›å»ºå¸¦é€æ˜èƒŒæ™¯çš„ç›®æ ‡å›¾åƒ"""
        height, width = frame_array.shape[:2]
        
        # åˆ›å»ºRGBAå›¾åƒ
        target_rgba = np.zeros((height, width, 4), dtype=np.uint8)
        target_rgba[:, :, :3] = frame_array
        target_rgba[:, :, 3] = (mask * 255).astype(np.uint8)
        
        return Image.fromarray(target_rgba, 'RGBA')
    
    def _replace_background(self, frame, mask, new_background):
        """æ‰§è¡ŒèƒŒæ™¯æ›¿æ¢"""
        frame_array = np.array(frame)
        
        # è°ƒæ•´æ–°èƒŒæ™¯å°ºå¯¸
        new_bg_resized = new_background.resize(frame.size)
        new_bg_array = np.array(new_bg_resized)
        
        # æ‰©å±•æ©ç åˆ°3ä¸ªé€šé“
        mask_3d = np.stack([mask, mask, mask], axis=2)
        
        # æ‰§è¡ŒèƒŒæ™¯æ›¿æ¢
        result = frame_array * mask_3d + new_bg_array * (1 - mask_3d)
        
        return Image.fromarray(result.astype(np.uint8))
    
    def _analyze_object_properties(self, mask):
        """åˆ†æç›®æ ‡å¯¹è±¡çš„å±æ€§"""
        # è®¡ç®—è´¨å¿ƒ
        y_coords, x_coords = np.where(mask)
        if len(x_coords) > 0:
            centroid = (int(np.mean(x_coords)), int(np.mean(y_coords)))
        else:
            centroid = (0, 0)
        
        # è®¡ç®—é¢ç§¯
        area = np.sum(mask)
        
        # è®¡ç®—è¾¹ç•Œæ¡†
        if len(x_coords) > 0:
            bbox = (int(np.min(x_coords)), int(np.min(y_coords)), 
                   int(np.max(x_coords)), int(np.max(y_coords)))
        else:
            bbox = (0, 0, 0, 0)
        
        return centroid, area, bbox
    
    def _create_tracking_visualization(self, frame, frame_results, frame_idx):
        """åˆ›å»ºå¸¦è·Ÿè¸ªä¿¡æ¯çš„å¯è§†åŒ–"""
        vis_frame = frame.copy()
        
        # é¢œè‰²æ˜ å°„
        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]
        
        for i, (obj_id, result) in enumerate(frame_results.items()):
            color = colors[i % len(colors)]
            centroid = result['centroid']
            bbox = result['bbox']
            confidence = result['confidence']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(vis_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
            
            # ç»˜åˆ¶è´¨å¿ƒ
            cv2.circle(vis_frame, centroid, 5, color, -1)
            
            # ç»˜åˆ¶è·Ÿè¸ªè½¨è¿¹
            if obj_id in self.tracking_history:
                positions = self.tracking_history[obj_id]['positions']
                if len(positions) > 1:
                    for j in range(1, len(positions)):
                        cv2.line(vis_frame, positions[j-1], positions[j], color, 2)
            
            # æ·»åŠ æ–‡æœ¬ä¿¡æ¯
            obj_name = self.tracking_history.get(obj_id, {}).get('name', f'Object {obj_id}')
            text = f"{obj_name}: {confidence:.2f}"
            cv2.putText(vis_frame, text, (bbox[0], bbox[1]-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        return vis_frame
    
    def _analyze_motion_trajectories(self):
        """åˆ†æè¿åŠ¨è½¨è¿¹"""
        motion_analysis = {}
        
        for obj_id, history in self.tracking_history.items():
            positions = history['positions']
            areas = history['areas']
            
            if len(positions) < 2:
                continue
            
            # è®¡ç®—è¿åŠ¨è·ç¦»
            total_distance = 0
            velocities = []
            
            for i in range(1, len(positions)):
                dx = positions[i][0] - positions[i-1][0]
                dy = positions[i][1] - positions[i-1][1]
                distance = np.sqrt(dx*dx + dy*dy)
                total_distance += distance
                velocities.append(distance)
            
            # è®¡ç®—è¿åŠ¨ç»Ÿè®¡
            avg_velocity = np.mean(velocities) if velocities else 0
            max_velocity = np.max(velocities) if velocities else 0
            
            # è®¡ç®—é¢ç§¯å˜åŒ–
            area_changes = []
            for i in range(1, len(areas)):
                change = abs(areas[i] - areas[i-1]) / areas[i-1] if areas[i-1] > 0 else 0
                area_changes.append(change)
            
            avg_area_change = np.mean(area_changes) if area_changes else 0
            
            motion_analysis[obj_id] = {
                'name': history['name'],
                'total_distance': total_distance,
                'average_velocity': avg_velocity,
                'max_velocity': max_velocity,
                'average_area_change': avg_area_change,
                'trajectory_length': len(positions),
                'start_position': positions[0] if positions else None,
                'end_position': positions[-1] if positions else None
            }
        
        return motion_analysis
    
    def _create_extraction_summary(self, frame_paths, extracted_targets, target_masks, output_dir, obj_name):
        """åˆ›å»ºç›®æ ‡æå–ç»“æœæ±‡æ€»"""
        fig, axes = plt.subplots(2, min(6, len(frame_paths)), figsize=(18, 8))
        if len(frame_paths) == 1:
            axes = axes.reshape(2, 1)
        
        for i in range(min(6, len(frame_paths))):
            # åŸå§‹å¸§
            frame = Image.open(frame_paths[i])
            axes[0, i].imshow(frame)
            axes[0, i].set_title(f"Frame {i}")
            axes[0, i].axis('off')
            
            # æå–çš„ç›®æ ‡
            if i < len(extracted_targets):
                target = Image.open(extracted_targets[i])
                axes[1, i].imshow(target)
                axes[1, i].set_title(f"Extracted {obj_name}")
            else:
                axes[1, i].axis('off')
            axes[1, i].axis('off')
        
        plt.suptitle(f"Target Extraction Results: {obj_name}", fontsize=16)
        plt.tight_layout()
        summary_path = os.path.join(output_dir, f"extraction_summary_{obj_name}.png")
        plt.savefig(summary_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æå–æ±‡æ€»å·²ä¿å­˜: {summary_path}")
    
    def _create_replacement_comparison(self, original_frames, replaced_frames, masks, output_dir):
        """åˆ›å»ºèƒŒæ™¯æ›¿æ¢å¯¹æ¯”å›¾"""
        fig, axes = plt.subplots(3, min(4, len(original_frames)), figsize=(16, 12))
        if len(original_frames) == 1:
            axes = axes.reshape(3, 1)
        
        for i in range(min(4, len(original_frames))):
            # åŸå§‹å¸§
            original = Image.open(original_frames[i])
            axes[0, i].imshow(original)
            axes[0, i].set_title(f"Original Frame {i}")
            axes[0, i].axis('off')
            
            # æ›¿æ¢åçš„å¸§
            if i < len(replaced_frames):
                replaced = Image.open(replaced_frames[i])
                axes[1, i].imshow(replaced)
                axes[1, i].set_title(f"Background Replaced")
            else:
                axes[1, i].axis('off')
            axes[1, i].axis('off')
            
            # æ©ç 
            if i in masks:
                axes[2, i].imshow(masks[i], cmap='gray')
                axes[2, i].set_title(f"Foreground Mask")
            else:
                axes[2, i].axis('off')
            axes[2, i].axis('off')
        
        plt.suptitle("Background Replacement Results", fontsize=16)
        plt.tight_layout()
        comparison_path = os.path.join(output_dir, "replacement_comparison.png")
        plt.savefig(comparison_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ›¿æ¢å¯¹æ¯”å›¾å·²ä¿å­˜: {comparison_path}")
    
    def _create_tracking_analysis(self, tracking_results, motion_analysis, output_dir):
        """åˆ›å»ºè·Ÿè¸ªåˆ†æå¯è§†åŒ–"""
        # åˆ›å»ºè½¨è¿¹å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # è½¨è¿¹å¯è§†åŒ–
        ax = axes[0, 0]
        colors = ['red', 'blue', 'green', 'orange', 'purple']
        
        for i, (obj_id, analysis) in enumerate(motion_analysis.items()):
            if obj_id in self.tracking_history:
                positions = self.tracking_history[obj_id]['positions']
                if len(positions) > 1:
                    x_coords = [pos[0] for pos in positions]
                    y_coords = [pos[1] for pos in positions]
                    color = colors[i % len(colors)]
                    ax.plot(x_coords, y_coords, color=color, marker='o', 
                           label=analysis['name'], linewidth=2, markersize=4)
        
        ax.set_title("Object Trajectories")
        ax.set_xlabel("X Position")
        ax.set_ylabel("Y Position")
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # é€Ÿåº¦åˆ†æ
        ax = axes[0, 1]
        obj_names = [analysis['name'] for analysis in motion_analysis.values()]
        avg_velocities = [analysis['average_velocity'] for analysis in motion_analysis.values()]
        
        ax.bar(obj_names, avg_velocities, color=['red', 'blue', 'green', 'orange', 'purple'][:len(obj_names)])
        ax.set_title("Average Velocity")
        ax.set_ylabel("Pixels per frame")
        ax.tick_params(axis='x', rotation=45)
        
        # é¢ç§¯å˜åŒ–
        ax = axes[1, 0]
        for i, (obj_id, history) in enumerate(self.tracking_history.items()):
            areas = history['areas']
            if areas:
                color = colors[i % len(colors)]
                ax.plot(range(len(areas)), areas, color=color, 
                       label=history['name'], linewidth=2)
        
        ax.set_title("Object Area Over Time")
        ax.set_xlabel("Frame")
        ax.set_ylabel("Area (pixels)")
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # ç½®ä¿¡åº¦åˆ†æ
        ax = axes[1, 1]
        for i, (obj_id, history) in enumerate(self.tracking_history.items()):
            scores = history['confidence_scores']
            if scores:
                color = colors[i % len(colors)]
                ax.plot(range(len(scores)), scores, color=color, 
                       label=history['name'], linewidth=2)
        
        ax.set_title("Tracking Confidence Over Time")
        ax.set_xlabel("Frame")
        ax.set_ylabel("Confidence Score")
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        analysis_path = os.path.join(output_dir, "tracking_analysis.png")
        plt.savefig(analysis_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è·Ÿè¸ªåˆ†æå›¾å·²ä¿å­˜: {analysis_path}")
    
    def _generate_extraction_report(self, frame_paths, target_masks, output_dir, obj_name, click_point):
        """ç”Ÿæˆç›®æ ‡æå–æŠ¥å‘Š"""
        report_content = f"""# ç›®æ ‡æå–æŠ¥å‘Š

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **ç›®æ ‡åç§°**: {obj_name}
- **å¤„ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ€»å¸§æ•°**: {len(frame_paths)}
- **åˆå§‹ç‚¹å‡»ä½ç½®**: {click_point}

## ğŸ“Š æå–ç»Ÿè®¡
- **æˆåŠŸæå–å¸§æ•°**: {len(target_masks)}
- **æˆåŠŸç‡**: {len(target_masks)/len(frame_paths)*100:.1f}%

## ğŸ¯ è´¨é‡åˆ†æ
"""
        
        if target_masks:
            areas = [np.sum(mask) for mask in target_masks.values()]
            avg_area = np.mean(areas)
            min_area = np.min(areas)
            max_area = np.max(areas)
            area_stability = 1 - (np.std(areas) / avg_area) if avg_area > 0 else 0
            
            report_content += f"""
- **å¹³å‡ç›®æ ‡é¢ç§¯**: {avg_area:.0f} åƒç´ 
- **é¢ç§¯èŒƒå›´**: {min_area:.0f} - {max_area:.0f} åƒç´ 
- **é¢ç§¯ç¨³å®šæ€§**: {area_stability:.3f}

## ğŸ’¡ åº”ç”¨å»ºè®®
- é«˜ç¨³å®šæ€§(>0.8): é€‚åˆç²¾ç¡®åº”ç”¨
- ä¸­ç­‰ç¨³å®šæ€§(0.5-0.8): é€‚åˆä¸€èˆ¬åº”ç”¨
- ä½ç¨³å®šæ€§(<0.5): å»ºè®®è°ƒæ•´å‚æ•°

## ğŸ“ è¾“å‡ºæ–‡ä»¶
- æå–çš„ç›®æ ‡å›¾åƒ: `{obj_name}_frame_*.png`
- æå–æ±‡æ€»å›¾: `extraction_summary_{obj_name}.png`
"""
        
        report_path = os.path.join(output_dir, f"extraction_report_{obj_name}.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“‹ æå–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _generate_replacement_report(self, frame_paths, masks, new_bg_path, output_dir, click_point):
        """ç”ŸæˆèƒŒæ™¯æ›¿æ¢æŠ¥å‘Š"""
        report_content = f"""# èƒŒæ™¯æ›¿æ¢æŠ¥å‘Š

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **å¤„ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ€»å¸§æ•°**: {len(frame_paths)}
- **æ–°èƒŒæ™¯**: {os.path.basename(new_bg_path)}
- **å‰æ™¯é€‰æ‹©ç‚¹**: {click_point}

## ğŸ“Š æ›¿æ¢ç»Ÿè®¡
- **æˆåŠŸæ›¿æ¢å¸§æ•°**: {len(masks)}
- **æˆåŠŸç‡**: {len(masks)/len(frame_paths)*100:.1f}%

## ğŸ¯ è´¨é‡åˆ†æ
"""
        
        if masks:
            # è®¡ç®—å‰æ™¯åŒºåŸŸç¨³å®šæ€§
            areas = [np.sum(mask) for mask in masks.values()]
            avg_area = np.mean(areas)
            area_stability = 1 - (np.std(areas) / avg_area) if avg_area > 0 else 0
            
            # è®¡ç®—è¾¹ç¼˜è´¨é‡ï¼ˆè¾¹ç¼˜åƒç´ æ¯”ä¾‹ï¼‰
            edge_qualities = []
            for mask in masks.values():
                edges = cv2.Canny((mask * 255).astype(np.uint8), 50, 150)
                edge_quality = np.sum(edges > 0) / np.sum(mask) if np.sum(mask) > 0 else 0
                edge_qualities.append(edge_quality)
            
            avg_edge_quality = np.mean(edge_qualities)
            
            report_content += f"""
- **å‰æ™¯é¢ç§¯ç¨³å®šæ€§**: {area_stability:.3f}
- **å¹³å‡è¾¹ç¼˜è´¨é‡**: {avg_edge_quality:.3f}
- **å¹³å‡å‰æ™¯å æ¯”**: {avg_area/np.prod(list(masks.values())[0].shape)*100:.1f}%

## ğŸ’¡ è´¨é‡è¯„ä¼°
- **ç¨³å®šæ€§è¯„çº§**: {"ä¼˜ç§€" if area_stability > 0.8 else "è‰¯å¥½" if area_stability > 0.5 else "ä¸€èˆ¬"}
- **è¾¹ç¼˜è´¨é‡**: {"æ¸…æ™°" if avg_edge_quality < 0.1 else "ä¸­ç­‰" if avg_edge_quality < 0.2 else "éœ€ä¼˜åŒ–"}

## ğŸ“ è¾“å‡ºæ–‡ä»¶
- æ›¿æ¢åè§†é¢‘å¸§: `replaced_frame_*.png`
- æ•ˆæœå¯¹æ¯”å›¾: `replacement_comparison.png`
"""
        
        report_path = os.path.join(output_dir, "replacement_report.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“‹ æ›¿æ¢æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _generate_tracking_report(self, tracking_results, motion_analysis, click_points, track_names, output_dir):
        """ç”Ÿæˆç›®æ ‡è·Ÿè¸ªæŠ¥å‘Š"""
        report_content = f"""# ç›®æ ‡è·Ÿè¸ªæŠ¥å‘Š

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯
- **å¤„ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **è·Ÿè¸ªç›®æ ‡æ•°**: {len(track_names)}
- **æ€»å¸§æ•°**: {len(tracking_results)}

## ğŸ¯ è·Ÿè¸ªç›®æ ‡
"""
        
        for i, (click_point, name) in enumerate(zip(click_points, track_names)):
            report_content += f"- **{name}**: åˆå§‹ä½ç½® {click_point}\n"
        
        report_content += f"""
## ğŸ“Š è¿åŠ¨åˆ†æ
"""
        
        for obj_id, analysis in motion_analysis.items():
            report_content += f"""
### {analysis['name']}
- **æ€»ç§»åŠ¨è·ç¦»**: {analysis['total_distance']:.1f} åƒç´ 
- **å¹³å‡é€Ÿåº¦**: {analysis['average_velocity']:.2f} åƒç´ /å¸§
- **æœ€å¤§é€Ÿåº¦**: {analysis['max_velocity']:.2f} åƒç´ /å¸§
- **å¹³å‡é¢ç§¯å˜åŒ–ç‡**: {analysis['average_area_change']:.3f}
- **è½¨è¿¹é•¿åº¦**: {analysis['trajectory_length']} å¸§
- **èµ·å§‹ä½ç½®**: {analysis['start_position']}
- **ç»“æŸä½ç½®**: {analysis['end_position']}
"""
        
        # è®¡ç®—è·Ÿè¸ªè´¨é‡æŒ‡æ ‡
        total_frames = len(tracking_results)
        successful_tracks = sum(1 for analysis in motion_analysis.values() 
                              if analysis['trajectory_length'] > total_frames * 0.8)
        
        report_content += f"""
## ğŸ† è·Ÿè¸ªè´¨é‡
- **è·Ÿè¸ªæˆåŠŸç‡**: {successful_tracks/len(track_names)*100:.1f}%
- **å¹³å‡è½¨è¿¹å®Œæ•´æ€§**: {np.mean([a['trajectory_length']/total_frames for a in motion_analysis.values()])*100:.1f}%

## ğŸ“ è¾“å‡ºæ–‡ä»¶
- è·Ÿè¸ªå¯è§†åŒ–å¸§: `tracked_frame_*.png`
- è¿åŠ¨åˆ†æå›¾: `tracking_analysis.png`
- è·Ÿè¸ªæ•°æ®: JSONæ ¼å¼ä¿å­˜åœ¨åŒç›®å½•

## ğŸ’¡ åˆ†æå»ºè®®
- é«˜é€Ÿè¿åŠ¨ç›®æ ‡å»ºè®®å¢åŠ é‡‡æ ·ç‡
- é®æŒ¡é¢‘ç¹åŒºåŸŸå¯è€ƒè™‘å¤šç‚¹æ ‡æ³¨
- é•¿æœŸè·Ÿè¸ªå»ºè®®å®šæœŸé‡æ–°æ ¡å‡†
"""
        
        report_path = os.path.join(output_dir, "tracking_report.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        # ä¿å­˜è·Ÿè¸ªæ•°æ®ä¸ºJSON
        tracking_data = {
            'motion_analysis': motion_analysis,
            'tracking_history': {str(k): v for k, v in self.tracking_history.items()},
            'summary': {
                'total_objects': len(track_names),
                'total_frames': total_frames,
                'successful_tracks': successful_tracks
            }
        }
        
        json_path = os.path.join(output_dir, "tracking_data.json")
        with open(json_path, 'w') as f:
            json.dump(tracking_data, f, indent=2, default=str)
        
        print(f"ğŸ“‹ è·Ÿè¸ªæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print(f"ğŸ“Š è·Ÿè¸ªæ•°æ®å·²ä¿å­˜: {json_path}")


def demo_advanced_video_applications():
    """æ¼”ç¤ºé«˜çº§è§†é¢‘åˆ†å‰²åº”ç”¨"""
    print("ğŸš€ SAM2 é«˜çº§è§†é¢‘åˆ†å‰²åº”ç”¨æ¼”ç¤º")
    print("=" * 60)
    
    # ç”±äºæ²¡æœ‰çœŸå®è§†é¢‘ï¼Œæˆ‘ä»¬åˆ›å»ºæ¨¡æ‹Ÿæ¼”ç¤º
    print("ğŸ“ åˆ›å»ºé«˜çº§åŠŸèƒ½æ¨¡æ‹Ÿæ¼”ç¤º...")
    
    # ä½¿ç”¨é™æ€å›¾ç‰‡æ¨¡æ‹Ÿä¸åŒåº”ç”¨
    demo_target_extraction_simulation()
    demo_background_replacement_simulation()
    demo_object_tracking_simulation()


def demo_target_extraction_simulation():
    """æ¨¡æ‹Ÿç›®æ ‡æå–æ¼”ç¤º"""
    print("\nğŸ¯ æ¨¡æ‹Ÿç›®æ ‡æå–æ¼”ç¤º")
    print("-" * 40)
    
    output_dir = "sam2_demos/target_extraction_demo"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä½¿ç”¨ç°æœ‰å›¾ç‰‡åˆ›å»ºæ¨¡æ‹Ÿæ•ˆæœ
    if not os.path.exists("notebooks/images/cars.jpg"):
        print("âŒ æ¼”ç¤ºå›¾ç‰‡ä¸å­˜åœ¨")
        return
    
    # è¯»å–åŸºç¡€å›¾ç‰‡
    base_image = Image.open("notebooks/images/cars.jpg")
    width, height = base_image.size
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„ç›®æ ‡æå–ç»“æœ
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # æ¨¡æ‹Ÿ3ä¸ªä¸åŒçš„æå–ç»“æœ
    for i in range(3):
        # åŸå§‹å›¾åƒ
        axes[0, i].imshow(base_image)
        axes[0, i].set_title(f"Frame {i+1}")
        axes[0, i].axis('off')
        
        # æ¨¡æ‹Ÿæå–çš„ç›®æ ‡ï¼ˆæ·»åŠ é€æ˜èƒŒæ™¯æ•ˆæœï¼‰
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¤­åœ†æ©ç ä½œä¸ºæ¼”ç¤º
        mask = Image.new('L', (width, height), 0)
        draw = ImageDraw.Draw(mask)
        center_x, center_y = width//2 + i*20, height//2 + i*10
        draw.ellipse([center_x-100, center_y-80, center_x+100, center_y+80], fill=255)
        
        # åº”ç”¨æ©ç åˆ›å»ºæå–æ•ˆæœ
        extracted = Image.new('RGBA', (width, height), (0, 0, 0, 0))
        extracted.paste(base_image, mask=mask)
        
        axes[1, i].imshow(extracted)
        axes[1, i].set_title(f"Extracted Target {i+1}")
        axes[1, i].axis('off')
    
    plt.suptitle("Target Extraction Simulation", fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    demo_path = os.path.join(output_dir, "target_extraction_demo.png")
    plt.savefig(demo_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    # åˆ›å»ºè¯´æ˜æ–‡æ¡£
    readme_content = """# ç›®æ ‡æå–åŠŸèƒ½æ¼”ç¤º

## ğŸ¯ åŠŸèƒ½æè¿°
ç›®æ ‡æå–åŠŸèƒ½å¯ä»¥ä»è§†é¢‘ä¸­ç²¾ç¡®æå–æŒ‡å®šçš„ç›®æ ‡å¯¹è±¡ï¼Œç”Ÿæˆå¸¦é€æ˜èƒŒæ™¯çš„ç›®æ ‡å›¾åƒã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§
- ç²¾ç¡®çš„ç›®æ ‡åˆ†å‰²å’Œæå–
- é€æ˜èƒŒæ™¯å¤„ç†
- å¤šå¸§ä¸€è‡´æ€§ä¿è¯
- è´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆ

## ğŸ”§ ä½¿ç”¨æ–¹æ³•
```python
segmenter = AdvancedVideoSegmenter()
extracted_targets, masks = segmenter.target_extraction(
    frame_paths=frame_list,
    click_point=[x, y],
    output_dir="output",
    obj_name="target_object"
)
```

## ğŸ’¡ åº”ç”¨åœºæ™¯
- è§†é¢‘ç´ ææå–
- å¯¹è±¡åˆ†æç ”ç©¶
- å†…å®¹åˆ›ä½œè¾…åŠ©
- åŠ¨ç”»åˆ¶ä½œç´ æ
"""
    
    readme_path = os.path.join(output_dir, "README.md")
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"âœ… ç›®æ ‡æå–æ¼”ç¤ºå·²ä¿å­˜: {demo_path}")


def demo_background_replacement_simulation():
    """æ¨¡æ‹ŸèƒŒæ™¯æ›¿æ¢æ¼”ç¤º"""
    print("\nğŸ¨ æ¨¡æ‹ŸèƒŒæ™¯æ›¿æ¢æ¼”ç¤º")
    print("-" * 40)
    
    output_dir = "sam2_demos/background_replacement_demo"
    os.makedirs(output_dir, exist_ok=True)
    
    if not os.path.exists("notebooks/images/cars.jpg"):
        print("âŒ æ¼”ç¤ºå›¾ç‰‡ä¸å­˜åœ¨")
        return
    
    # è¯»å–åŸºç¡€å›¾ç‰‡
    base_image = Image.open("notebooks/images/cars.jpg")
    width, height = base_image.size
    
    # åˆ›å»ºå‡ ä¸ªä¸åŒçš„èƒŒæ™¯
    backgrounds = []
    
    # æ¸å˜èƒŒæ™¯
    gradient_bg = Image.new('RGB', (width, height))
    for y in range(height):
        color = int(255 * y / height)
        for x in range(width):
            gradient_bg.putpixel((x, y), (color, 100, 255-color))
    backgrounds.append(("Gradient", gradient_bg))
    
    # çº¯è‰²èƒŒæ™¯
    solid_bg = Image.new('RGB', (width, height), (50, 150, 50))
    backgrounds.append(("Green Screen", solid_bg))
    
    # çº¹ç†èƒŒæ™¯
    texture_bg = Image.new('RGB', (width, height))
    for y in range(height):
        for x in range(width):
            noise = (x + y) % 50
            texture_bg.putpixel((x, y), (100 + noise, 120 + noise, 140 + noise))
    backgrounds.append(("Texture", texture_bg))
    
    # åˆ›å»ºæ›¿æ¢æ•ˆæœå±•ç¤º
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    for i, (bg_name, bg_image) in enumerate(backgrounds):
        # åŸå§‹å›¾åƒ
        axes[0, i].imshow(base_image)
        axes[0, i].set_title("Original")
        axes[0, i].axis('off')
        
        # æ¨¡æ‹ŸèƒŒæ™¯æ›¿æ¢æ•ˆæœ
        # åˆ›å»ºç®€å•çš„å‰æ™¯æ©ç 
        mask = Image.new('L', (width, height), 0)
        draw = ImageDraw.Draw(mask)
        draw.ellipse([width//4, height//4, width*3//4, height*3//4], fill=255)
        
        # æ‰§è¡Œç®€å•çš„èƒŒæ™¯æ›¿æ¢
        result = Image.composite(base_image, bg_image, mask)
        
        axes[1, i].imshow(result)
        axes[1, i].set_title(f"With {bg_name} Background")
        axes[1, i].axis('off')
    
    plt.suptitle("Background Replacement Simulation", fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    demo_path = os.path.join(output_dir, "background_replacement_demo.png")
    plt.savefig(demo_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    # åˆ›å»ºè¯´æ˜æ–‡æ¡£
    readme_content = """# èƒŒæ™¯æ›¿æ¢åŠŸèƒ½æ¼”ç¤º

## ğŸ¨ åŠŸèƒ½æè¿°
èƒŒæ™¯æ›¿æ¢åŠŸèƒ½å¯ä»¥å°†è§†é¢‘ä¸­çš„èƒŒæ™¯æ›¿æ¢ä¸ºä»»æ„æŒ‡å®šçš„æ–°èƒŒæ™¯ï¼Œä¿æŒå‰æ™¯å¯¹è±¡ä¸å˜ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§
- ç²¾ç¡®çš„å‰æ™¯/èƒŒæ™¯åˆ†ç¦»
- æ”¯æŒä»»æ„èƒŒæ™¯å›¾åƒ
- è¾¹ç¼˜ä¼˜åŒ–å¤„ç†
- å¤šå¸§æ—¶åºä¸€è‡´æ€§

## ğŸ”§ ä½¿ç”¨æ–¹æ³•
```python
segmenter = AdvancedVideoSegmenter()
replaced_frames, masks = segmenter.background_replacement(
    frame_paths=frame_list,
    click_point=[x, y],  # ç‚¹å‡»å‰æ™¯å¯¹è±¡
    new_background_path="new_bg.jpg",
    output_dir="output"
)
```

## ğŸ’¡ åº”ç”¨åœºæ™¯
- è™šæ‹ŸèƒŒæ™¯ä¼šè®®
- å½±è§†åæœŸåˆ¶ä½œ
- ç›´æ’­èƒŒæ™¯æ›¿æ¢
- åˆ›æ„è§†é¢‘åˆ¶ä½œ
- äº§å“å±•ç¤ºè§†é¢‘
"""
    
    readme_path = os.path.join(output_dir, "README.md")
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"âœ… èƒŒæ™¯æ›¿æ¢æ¼”ç¤ºå·²ä¿å­˜: {demo_path}")


def demo_object_tracking_simulation():
    """æ¨¡æ‹Ÿç›®æ ‡è·Ÿè¸ªæ¼”ç¤º"""
    print("\nğŸ” æ¨¡æ‹Ÿç›®æ ‡è·Ÿè¸ªæ¼”ç¤º")
    print("-" * 40)
    
    output_dir = "sam2_demos/object_tracking_demo"
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„å¤šç›®æ ‡è·Ÿè¸ªè½¨è¿¹
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # æ¨¡æ‹Ÿæ•°æ®
    frames = 20
    
    # ç›®æ ‡1è½¨è¿¹ - ç›´çº¿è¿åŠ¨
    traj1_x = np.linspace(100, 400, frames)
    traj1_y = np.linspace(100, 200, frames)
    
    # ç›®æ ‡2è½¨è¿¹ - åœ†å½¢è¿åŠ¨
    angles = np.linspace(0, 2*np.pi, frames)
    traj2_x = 250 + 100 * np.cos(angles)
    traj2_y = 250 + 100 * np.sin(angles)
    
    # ç›®æ ‡3è½¨è¿¹ - éšæœºè¿åŠ¨
    np.random.seed(42)
    traj3_x = 300 + np.cumsum(np.random.randn(frames) * 10)
    traj3_y = 150 + np.cumsum(np.random.randn(frames) * 10)
    
    # ç»˜åˆ¶è½¨è¿¹
    ax = axes[0, 0]
    ax.plot(traj1_x, traj1_y, 'ro-', label='Target 1 (Linear)', linewidth=2, markersize=6)
    ax.plot(traj2_x, traj2_y, 'bo-', label='Target 2 (Circular)', linewidth=2, markersize=6)
    ax.plot(traj3_x, traj3_y, 'go-', label='Target 3 (Random)', linewidth=2, markersize=6)
    ax.set_title("Object Trajectories")
    ax.set_xlabel("X Position")
    ax.set_ylabel("Y Position")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # é€Ÿåº¦åˆ†æ
    ax = axes[0, 1]
    vel1 = np.sqrt(np.diff(traj1_x)**2 + np.diff(traj1_y)**2)
    vel2 = np.sqrt(np.diff(traj2_x)**2 + np.diff(traj2_y)**2)
    vel3 = np.sqrt(np.diff(traj3_x)**2 + np.diff(traj3_y)**2)
    
    ax.plot(vel1, 'r-', label='Target 1', linewidth=2)
    ax.plot(vel2, 'b-', label='Target 2', linewidth=2)
    ax.plot(vel3, 'g-', label='Target 3', linewidth=2)
    ax.set_title("Velocity Over Time")
    ax.set_xlabel("Frame")
    ax.set_ylabel("Velocity (pixels/frame)")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # è·ç¦»åˆ†æ
    ax = axes[1, 0]
    dist1 = np.cumsum(vel1)
    dist2 = np.cumsum(vel2)
    dist3 = np.cumsum(vel3)
    
    ax.plot(dist1, 'r-', label='Target 1', linewidth=2)
    ax.plot(dist2, 'b-', label='Target 2', linewidth=2)
    ax.plot(dist3, 'g-', label='Target 3', linewidth=2)
    ax.set_title("Cumulative Distance")
    ax.set_xlabel("Frame")
    ax.set_ylabel("Total Distance (pixels)")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # ç»Ÿè®¡ä¿¡æ¯
    ax = axes[1, 1]
    targets = ['Target 1', 'Target 2', 'Target 3']
    avg_velocities = [np.mean(vel1), np.mean(vel2), np.mean(vel3)]
    colors = ['red', 'blue', 'green']
    
    bars = ax.bar(targets, avg_velocities, color=colors, alpha=0.7)
    ax.set_title("Average Velocity Comparison")
    ax.set_ylabel("Average Velocity (pixels/frame)")
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, vel in zip(bars, avg_velocities):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                f'{vel:.1f}', ha='center', va='bottom')
    
    plt.tight_layout()
    
    demo_path = os.path.join(output_dir, "object_tracking_demo.png")
    plt.savefig(demo_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    # åˆ›å»ºè¯´æ˜æ–‡æ¡£
    readme_content = """# ç›®æ ‡è·Ÿè¸ªåŠŸèƒ½æ¼”ç¤º

## ğŸ” åŠŸèƒ½æè¿°
ç›®æ ‡è·Ÿè¸ªåŠŸèƒ½æ”¯æŒåŒæ—¶è·Ÿè¸ªå¤šä¸ªç›®æ ‡å¯¹è±¡ï¼Œåˆ†æå…¶è¿åŠ¨è½¨è¿¹ã€é€Ÿåº¦å˜åŒ–ç­‰è¿åŠ¨ç‰¹å¾ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§
- å¤šç›®æ ‡åŒæ—¶è·Ÿè¸ª
- å®æ—¶è½¨è¿¹åˆ†æ
- è¿åŠ¨ç‰¹å¾ç»Ÿè®¡
- é®æŒ¡æ¢å¤èƒ½åŠ›
- è‡ªåŠ¨è´¨é‡è¯„ä¼°

## ğŸ”§ ä½¿ç”¨æ–¹æ³•
```python
segmenter = AdvancedVideoSegmenter()
tracked_frames, results, motion_analysis = segmenter.object_tracking(
    frame_paths=frame_list,
    click_points=[(x1, y1), (x2, y2), ...],  # å¤šä¸ªç›®æ ‡ç‚¹
    output_dir="output",
    track_names=["target1", "target2", ...]
)
```

## ğŸ“Š åˆ†ææŒ‡æ ‡
- **è½¨è¿¹å®Œæ•´æ€§**: è·Ÿè¸ªæˆåŠŸçš„å¸§æ•°æ¯”ä¾‹
- **è¿åŠ¨é€Ÿåº¦**: å¹³å‡é€Ÿåº¦å’Œæœ€å¤§é€Ÿåº¦
- **è¿åŠ¨è·ç¦»**: æ€»ç§»åŠ¨è·ç¦»
- **ç½®ä¿¡åº¦**: è·Ÿè¸ªè´¨é‡è¯„åˆ†
- **ç›®æ ‡ç¨³å®šæ€§**: å¤§å°å˜åŒ–ç‡

## ğŸ’¡ åº”ç”¨åœºæ™¯
- è¿åŠ¨åˆ†æç ”ç©¶
- å®‰é˜²ç›‘æ§ç³»ç»Ÿ
- ä½“è‚²æ¯”èµ›åˆ†æ
- äº¤é€šæµé‡ç›‘æµ‹
- è¡Œä¸ºæ¨¡å¼ç ”ç©¶
"""
    
    readme_path = os.path.join(output_dir, "README.md")
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"âœ… ç›®æ ‡è·Ÿè¸ªæ¼”ç¤ºå·²ä¿å­˜: {demo_path}")


if __name__ == "__main__":
    demo_advanced_video_applications()
