#!/usr/bin/env python3
"""
SAM2 è§†é¢‘åˆ†å‰²åº”ç”¨
æ”¯æŒè§†é¢‘ä¸­çš„å¯¹è±¡è¿½è¸ªå’Œåˆ†å‰²
"""

import torch
import numpy as np
from PIL import Image
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import os
import cv2
from sam2.build_sam import build_sam2_video_predictor
from sam2.sam2_video_predictor import SAM2VideoPredictor

class VideoSegmenter:
    def __init__(self, model_path="checkpoints/sam2.1_hiera_tiny.pt"):
        """åˆå§‹åŒ–è§†é¢‘åˆ†å‰²å™¨"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ åˆå§‹åŒ–SAM2è§†é¢‘åˆ†å‰²å™¨ (è®¾å¤‡: {self.device})")
        
        # åŠ è½½è§†é¢‘é¢„æµ‹å™¨
        self.predictor = SAM2VideoPredictor.from_pretrained(
            model_id="facebook/sam2-hiera-tiny",
            device=self.device
        )
        print("âœ… è§†é¢‘æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def extract_frames(self, video_path, output_dir, max_frames=30):
        """ä»è§†é¢‘æå–å¸§"""
        os.makedirs(output_dir, exist_ok=True)
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.1f} FPS")
        
        # è®¡ç®—é‡‡æ ·é—´éš”
        frame_interval = max(1, total_frames // max_frames)
        extracted_frames = []
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:
                # è½¬æ¢ä¸ºRGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_path = os.path.join(output_dir, f"frame_{len(extracted_frames):04d}.jpg")
                Image.fromarray(frame_rgb).save(frame_path)
                extracted_frames.append(frame_path)
                
                if len(extracted_frames) >= max_frames:
                    break
            
            frame_count += 1
        
        cap.release()
        print(f"âœ… æå–äº† {len(extracted_frames)} å¸§")
        return extracted_frames, fps
    
    def segment_video_object(self, frame_paths, click_point, output_dir):
        """å¯¹è§†é¢‘ä¸­çš„å¯¹è±¡è¿›è¡Œåˆ†å‰²å’Œè¿½è¸ª"""
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ¯ å¼€å§‹è§†é¢‘å¯¹è±¡åˆ†å‰²...")
        print(f"   ç‚¹å‡»ä½ç½®: {click_point}")
        print(f"   å¸§æ•°: {len(frame_paths)}")
        
        # åˆå§‹åŒ–æ¨ç†çŠ¶æ€
        inference_state = self.predictor.init_state(video_path=os.path.dirname(frame_paths[0]))
        
        # åœ¨ç¬¬ä¸€å¸§æ·»åŠ ç‚¹å‡»ç‚¹
        ann_frame_idx = 0  # åœ¨ç¬¬ä¸€å¸§æ·»åŠ æ³¨é‡Š
        ann_obj_id = 1     # å¯¹è±¡ID
        
        # è¯»å–ç¬¬ä¸€å¸§
        first_frame = np.array(Image.open(frame_paths[0]))
        
        # æ·»åŠ ç‚¹å‡»ç‚¹
        points = np.array([click_point], dtype=np.float32)
        labels = np.array([1], np.int32)  # 1è¡¨ç¤ºå‰æ™¯ç‚¹
        
        _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(
            inference_state=inference_state,
            frame_idx=ann_frame_idx,
            obj_id=ann_obj_id,
            points=points,
            labels=labels,
        )
        
        print(f"âœ… åœ¨ç¬¬ä¸€å¸§æ·»åŠ äº†è¿½è¸ªç‚¹")
        
        # ä¼ æ’­åˆ°æ‰€æœ‰å¸§
        video_segments = {}
        for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(inference_state):
            video_segments[out_frame_idx] = {
                out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()
                for i, out_obj_id in enumerate(out_obj_ids)
            }
        
        print(f"âœ… å®Œæˆè§†é¢‘ä¼ æ’­ï¼Œå¤„ç†äº† {len(video_segments)} å¸§")
        
        # å¯è§†åŒ–ç»“æœ
        self._visualize_video_results(frame_paths, video_segments, click_point, output_dir)
        
        return video_segments
    
    def _visualize_video_results(self, frame_paths, video_segments, click_point, output_dir):
        """å¯è§†åŒ–è§†é¢‘åˆ†å‰²ç»“æœ"""
        print("ğŸ¨ ç”Ÿæˆè§†é¢‘åˆ†å‰²å¯è§†åŒ–...")
        
        # åˆ›å»ºç»“æœå¸§
        result_frames = []
        
        for i, frame_path in enumerate(frame_paths):
            frame = np.array(Image.open(frame_path))
            
            if i in video_segments:
                masks = video_segments[i]
                
                # åˆ›å»ºå¯è§†åŒ–
                fig, axes = plt.subplots(1, 3, figsize=(15, 5))
                
                # åŸå§‹å¸§
                axes[0].imshow(frame)
                if i == 0:  # åœ¨ç¬¬ä¸€å¸§æ˜¾ç¤ºç‚¹å‡»ç‚¹
                    axes[0].plot(click_point[0], click_point[1], 'ro', markersize=10)
                axes[0].set_title(f"Frame {i}")
                axes[0].axis('off')
                
                # åˆ†å‰²ç»“æœ
                axes[1].imshow(frame)
                for obj_id, mask in masks.items():
                    axes[1].imshow(mask, alpha=0.5, cmap='jet')
                axes[1].set_title(f"Segmentation")
                axes[1].axis('off')
                
                # çº¯mask
                for obj_id, mask in masks.items():
                    axes[2].imshow(mask, cmap='gray')
                axes[2].set_title("Mask")
                axes[2].axis('off')
                
                # ä¿å­˜å¸§ç»“æœ
                frame_output = os.path.join(output_dir, f"result_frame_{i:04d}.png")
                plt.tight_layout()
                plt.savefig(frame_output, dpi=100, bbox_inches='tight')
                plt.close()
                
                result_frames.append(frame_output)
        
        print(f"âœ… ç”Ÿæˆäº† {len(result_frames)} ä¸ªç»“æœå¸§")
        
        # åˆ›å»ºæ±‡æ€»å›¾
        self._create_video_summary(result_frames[:12], output_dir)  # æœ€å¤šæ˜¾ç¤º12å¸§
    
    def _create_video_summary(self, frame_paths, output_dir):
        """åˆ›å»ºè§†é¢‘åˆ†å‰²æ±‡æ€»å›¾"""
        if not frame_paths:
            return
        
        num_frames = len(frame_paths)
        cols = min(4, num_frames)
        rows = (num_frames + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 3))
        if rows == 1:
            axes = [axes] if cols == 1 else axes
        else:
            axes = axes.flatten()
        
        for i, frame_path in enumerate(frame_paths):
            if i < len(axes):
                img = Image.open(frame_path)
                axes[i].imshow(img)
                axes[i].set_title(f"Frame {i}")
                axes[i].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(frame_paths), len(axes)):
            axes[i].axis('off')
        
        summary_path = os.path.join(output_dir, "video_segmentation_summary.png")
        plt.tight_layout()
        plt.savefig(summary_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è§†é¢‘æ±‡æ€»å›¾å·²ä¿å­˜: {summary_path}")

def demo_video_segmentation():
    """æ¼”ç¤ºè§†é¢‘åˆ†å‰²ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰"""
    print("ğŸš€ SAM2 è§†é¢‘åˆ†å‰²æ¼”ç¤º")
    print("=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘æ–‡ä»¶
    video_path = "notebooks/videos/bedroom.mp4"
    if not os.path.exists(video_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        print("ğŸ“ åˆ›å»ºæ¨¡æ‹Ÿè§†é¢‘åˆ†å‰²æ¼”ç¤º...")
        demo_simulated_video_segmentation()
        return
    
    try:
        # åˆå§‹åŒ–è§†é¢‘åˆ†å‰²å™¨
        segmenter = VideoSegmenter()
        
        # æå–è§†é¢‘å¸§
        frames_dir = "sam2_demos/video_frames"
        frame_paths, fps = segmenter.extract_frames(video_path, frames_dir, max_frames=20)
        
        # å®šä¹‰ç‚¹å‡»ç‚¹ï¼ˆè§†é¢‘ä¸­å¿ƒï¼‰
        first_frame = Image.open(frame_paths[0])
        width, height = first_frame.size
        click_point = [width // 2, height // 2]
        
        # æ‰§è¡Œè§†é¢‘åˆ†å‰²
        output_dir = "sam2_demos/video_results"
        video_segments = segmenter.segment_video_object(frame_paths, click_point, output_dir)
        
        print("âœ… è§†é¢‘åˆ†å‰²æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è§†é¢‘åˆ†å‰²æ¼”ç¤ºå¤±è´¥: {e}")
        print("ğŸ“ å°è¯•æ¨¡æ‹Ÿæ¼”ç¤º...")
        demo_simulated_video_segmentation()

def demo_simulated_video_segmentation():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„è§†é¢‘åˆ†å‰²æ¼”ç¤º"""
    print("ğŸ¬ åˆ›å»ºæ¨¡æ‹Ÿè§†é¢‘åˆ†å‰²æ¼”ç¤º...")
    
    output_dir = "sam2_demos/simulated_video"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä½¿ç”¨é™æ€å›¾ç‰‡æ¨¡æ‹Ÿè§†é¢‘å¸§
    image_path = "notebooks/images/cars.jpg"
    if not os.path.exists(image_path):
        print(f"âŒ åŸºç¡€å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
        return
    
    # è¯»å–åŸºç¡€å›¾ç‰‡
    base_image = Image.open(image_path)
    width, height = base_image.size
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„"è§†é¢‘å¸§"ï¼ˆæ·»åŠ ä¸åŒçš„å˜æ¢ï¼‰
    frames = []
    for i in range(8):
        # åº”ç”¨è½»å¾®çš„å˜æ¢æ¨¡æ‹Ÿè¿åŠ¨
        angle = i * 2  # è½»å¾®æ—‹è½¬
        scale = 1.0 + i * 0.01  # è½»å¾®ç¼©æ”¾
        
        # åˆ›å»ºå˜æ¢åçš„å›¾ç‰‡
        transformed = base_image.rotate(angle, expand=False)
        # è½»å¾®è°ƒæ•´äº®åº¦
        enhanced = Image.eval(transformed, lambda x: min(255, int(x * (0.9 + i * 0.02))))
        
        frame_path = os.path.join(output_dir, f"simulated_frame_{i:04d}.jpg")
        enhanced.save(frame_path)
        frames.append(frame_path)
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„åˆ†å‰²ç»“æœå±•ç¤º
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    
    for i, frame_path in enumerate(frames):
        row = i // 4
        col = i % 4
        
        frame = Image.open(frame_path)
        axes[row, col].imshow(frame)
        
        # æ¨¡æ‹Ÿæ·»åŠ åˆ†å‰²æ•ˆæœ
        if i == 0:
            # åœ¨ç¬¬ä¸€å¸§æ˜¾ç¤ºç‚¹å‡»ç‚¹
            axes[row, col].plot(width//2, height//2, 'ro', markersize=8)
            axes[row, col].set_title(f"Frame {i} (Click Point)")
        else:
            # åœ¨å…¶ä»–å¸§æ˜¾ç¤ºæ¨¡æ‹Ÿçš„åˆ†å‰²åŒºåŸŸ
            circle = plt.Circle((width//2, height//2), 50 + i*10, 
                              color='red', fill=False, linewidth=3, alpha=0.7)
            axes[row, col].add_patch(circle)
            axes[row, col].set_title(f"Frame {i} (Tracked)")
        
        axes[row, col].axis('off')
    
    plt.suptitle("SAM2 Video Segmentation Simulation", fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    summary_path = os.path.join(output_dir, "simulated_video_segmentation.png")
    plt.savefig(summary_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… æ¨¡æ‹Ÿè§†é¢‘åˆ†å‰²æ¼”ç¤ºå·²ä¿å­˜: {summary_path}")
    
    # åˆ›å»ºè¯´æ˜æ–‡æ¡£
    readme_content = """# æ¨¡æ‹Ÿè§†é¢‘åˆ†å‰²æ¼”ç¤º

## ğŸ“ è¯´æ˜
ç”±äºç¼ºå°‘å®é™…è§†é¢‘æ–‡ä»¶ï¼Œæ­¤æ¼”ç¤ºå±•ç¤ºäº†SAM2è§†é¢‘åˆ†å‰²çš„å·¥ä½œåŸç†ï¼š

## ğŸ¯ è§†é¢‘åˆ†å‰²æµç¨‹
1. **åˆå§‹æ ‡æ³¨**: åœ¨ç¬¬ä¸€å¸§ç‚¹å‡»ç›®æ ‡å¯¹è±¡
2. **è‡ªåŠ¨ä¼ æ’­**: SAM2è‡ªåŠ¨è¿½è¸ªå¯¹è±¡åˆ°åç»­å¸§
3. **ä¸€è‡´æ€§ç»´æŠ¤**: ä¿æŒåˆ†å‰²çš„æ—¶åºä¸€è‡´æ€§

## ğŸ”§ å®é™…ä½¿ç”¨
è¦ä½¿ç”¨çœŸå®è§†é¢‘åˆ†å‰²åŠŸèƒ½ï¼š
1. å°†è§†é¢‘æ–‡ä»¶æ”¾å…¥ `notebooks/videos/` ç›®å½•
2. è¿è¡Œ `python video_segmentation.py`
3. SAM2å°†è‡ªåŠ¨æå–å¸§å¹¶è¿›è¡Œåˆ†å‰²

## ğŸ’¡ åº”ç”¨åœºæ™¯
- è§†é¢‘å¯¹è±¡ç§»é™¤
- è¿åŠ¨åˆ†æ
- è§†é¢‘ç¼–è¾‘
- è¡Œä¸ºè¿½è¸ª
"""
    
    readme_path = os.path.join(output_dir, "README.md")
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"ğŸ“‹ è¯´æ˜æ–‡æ¡£å·²åˆ›å»º: {readme_path}")

if __name__ == "__main__":
    demo_video_segmentation()
